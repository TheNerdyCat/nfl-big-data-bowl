{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# NFL Big Data Bowl - Feature Engineering v1\n-------------------\nTheNerdyCat <br>\nCreated: 2019/10/27 <br>\nDeadline: 2019/11/27"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n#import matplotlib.patches as patches\nimport seaborn as sns\n\nimport kaggle\nimport math\nimport datetime\nimport random\nfrom string import punctuation\nimport re\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold, GroupKFold, train_test_split\n\n#from sklearn.model_selection import GridSearchCV\n#import lightgbm as lgb\nimport keras\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom keras.utils import plot_model\nimport keras.backend as K\nimport tensorflow as tf\nimport gc\n#import optuna\n\nfrom sklearn.metrics import mean_absolute_error\n\nimport tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set_style('darkgrid')\nmpl.rcParams['figure.figsize'] = [15,10]\npd.options.display.max_columns = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/nfl-big-data-bowl-2020/train.csv\", low_memory=False)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df):\n\n    Turf = {'Field Turf':'Artificial', 'A-Turf Titan':'Artificial', 'Grass':'Natural', 'UBU Sports Speed S5-M':'Artificial', \n            'Artificial':'Artificial', 'DD GrassMaster':'Artificial', 'Natural Grass':'Natural', \n            'UBU Speed Series-S5-M':'Artificial', 'FieldTurf':'Artificial', 'FieldTurf 360':'Artificial', 'Natural grass':'Natural', 'grass':'Natural', \n            'Natural':'Natural', 'Artifical':'Artificial', 'FieldTurf360':'Artificial', 'Naturall Grass':'Natural', 'Field turf':'Artificial', \n            'SISGrass':'Artificial', 'Twenty-Four/Seven Turf':'Artificial', 'natural grass':'Natural'} \n    \n    # from https://www.kaggle.com/bgmello/neural-networks-feature-engineering-for-the-win\n    map_abbr = {'ARI': 'ARZ', 'BAL': 'BLT', 'CLE': 'CLV', 'HOU': 'HST'}\n    for abb in df['PossessionTeam'].unique():\n        map_abbr[abb] = abb\n    \n    def clean_position(pos):\n        if pos == 'SAF':\n            return 'DB'\n        if pos == 'S':\n            return 'DB'\n        elif pos == 'OG':\n            return 'G'\n        elif pos == \"OT\":\n            return 'T'\n        else:\n            return pos\n    \n    def clean_offenceformation(of):\n        if of == \"SHOTGUN\":\n            return 9\n        elif of == \"SINGLEBACK\":\n            return 8\n        elif of == \"JUMBO\":\n            return 6\n        elif of == \"PISTOL\":\n            return 5\n        elif of == \"I_FORM\":\n            return 4\n        elif of == \"ACE\":\n            return 3\n        elif of ==  \"WILDCAT\":\n            return 2\n        elif of == \"EMPTY\":\n            return 1\n        else: \n            return 7\n    \n    def create_generalposition(pos):\n        if pos == 'SS' or pos == 'FS' or pos == 'CB' or pos == 'DB':\n            return 'DB'\n        elif pos == 'DE' or pos == 'DT' or pos == 'DL':\n            return 'DL'\n        elif pos == 'ILB' or pos == 'OLB' or pos == 'MLB' or pos == 'LB':\n            return 'LB'\n        elif pos == 'WR':\n            return 'WR'\n        elif pos == 'TE':\n            return 'TE'\n        elif pos == 'T' or pos == 'G' or pos == 'C' or pos == 'NT' or pos == 'OL':\n            return 'OL'\n        elif pos == 'QB' or pos == 'RB' or pos == 'FB' or pos == 'HB' or pos == 'TB' or pos == 'WB':\n            return 'OB'\n        else:\n            return 'Other'\n    \n    def utc2sec(x):\n        return int(x.split(\"-\")[2].split(\":\")[2].split(\".\")[0])\n    def timesnap2date(x):\n        day = x.split(\"T\")[0]\n        return day\n    def timesnap2day(x):\n        days = x.split(\"-\")\n        return 365 * int(days[0]) + 30 * int(days[1]) + int(days[2][:2])\n    def gameclock2secs(x):\n        clock = x.split(\":\")\n        return (60 * int(clock[0])) + int(clock[1])        \n        \n    def group_stadium_types(stadium):\n        outdoor = [\n            'Outdoor', 'Outdoors', 'Cloudy', 'Heinz Field', 'Outdor', 'Ourdoor', \n            'Outside', 'Outddors','Outdoor Retr Roof-Open', 'Oudoor', 'Bowl'\n            ]\n        indoor_closed = [\n            'Indoors', 'Indoor', 'Indoor, Roof Closed', 'Indoor, Roof Closed',\n            'Retractable Roof', 'Retr. Roof-Closed', 'Retr. Roof - Closed', 'Retr. Roof Closed',\n        ]\n        indoor_open   = ['Indoor, Open Roof', 'Open', 'Retr. Roof-Open', 'Retr. Roof - Open']\n        dome_closed   = ['Dome', 'Domed, closed', 'Closed Dome', 'Domed', 'Dome, closed']\n        dome_open     = ['Domed, Open', 'Domed, open']\n        if stadium in outdoor:\n            return 0 #'outdoor'\n        elif stadium in indoor_closed:\n            return 3 # 'indoor closed'\n        elif stadium in indoor_open:\n            return 2 #'indoor open'\n        elif stadium in dome_closed:\n            return 4 #'dome closed'\n        elif stadium in dome_open:\n            return 1 #'dome open'\n        else:\n            return 5 #'unknown'\n        \n    def map_weather(txt):\n        ans = 1\n        if pd.isna(txt):\n            return 0\n        if 'partly' in txt:\n            ans*=0.5\n        if 'climate controlled' in txt or 'indoor' in txt:\n            return ans*3\n        if 'sunny' in txt or 'sun' in txt:\n            return ans*2\n        if 'clear' in txt:\n            return ans\n        if 'cloudy' in txt:\n            return -ans\n        if 'rain' in txt or 'rainy' in txt:\n            return -2*ans\n        if 'snow' in txt:\n            return -3*ans\n        return 0       \n        \n    def clean_wind_speed(windspeed):\n        \"\"\"\n        This is not a very robust function,\n        but it should do the job for this dataset.\n        \"\"\"\n        ws = str(windspeed)\n        # if it's already a number just return an int value\n        if ws.isdigit():\n            return int(ws)\n        # if it's a range, take their mean\n        if '-' in ws:\n            return (int(ws.split('-')[0]) + int(ws.split('-')[1]))/2\n        # if there's a space between the number and mph\n        if ws.split(' ')[0].isdigit():\n            return int(ws.split(' ')[0])\n        # if it looks like '10MPH' or '12mph' just take the first part\n        if 'mph' in ws.lower():\n            return int(ws.lower().split('mph')[0])\n        else:\n            return 0   \n            \n    def clean_WindDirection(txt):\n        if pd.isna(txt):\n            return np.nan\n        txt = txt.lower()\n        txt = ''.join([c for c in txt if c not in punctuation])\n        txt = txt.replace('from', '')\n        txt = txt.replace(' ', '')\n        txt = txt.replace('north', 'n')\n        txt = txt.replace('south', 's')\n        txt = txt.replace('west', 'w')\n        txt = txt.replace('east', 'e')\n        return txt         \n        \n    def transform_WindDirection(txt):\n        if pd.isna(txt):\n            return np.nan\n        \n        if txt=='n':\n            return 0\n        if txt=='nne' or txt=='nen':\n            return 1/8\n        if txt=='ne':\n            return 2/8\n        if txt=='ene' or txt=='nee':\n            return 3/8\n        if txt=='e':\n            return 4/8\n        if txt=='ese' or txt=='see':\n            return 5/8\n        if txt=='se':\n            return 6/8\n        if txt=='ses' or txt=='sse':\n            return 7/8\n        if txt=='s':\n            return 8/8\n        if txt=='ssw' or txt=='sws':\n            return 9/8\n        if txt=='sw':\n            return 10/8\n        if txt=='sww' or txt=='wsw':\n            return 11/8\n        if txt=='w':\n            return 12/8\n        if txt=='wnw' or txt=='nww':\n            return 13/8\n        if txt=='nw':\n            return 14/8\n        if txt=='nwn' or txt=='nnw':\n            return 15/8\n        return np.nan\n        \n    def birthday2day(x):\n        days = x.split(\"/\")\n        return 30 * int(days[0]) + int(days[1]) + 365 * int(days[2])\n    \n    def height2inch(x):\n        height = x.split(\"-\")\n        return 12 * int(height[0]) + int(height[1])    \n    \n    def str_to_float(txt):\n        try:\n            return float(txt)\n        except:\n            return -1\n    df['PossessionTeam'] = df['PossessionTeam'].map(map_abbr)\n    df['HomeTeamAbbr'] = df['HomeTeamAbbr'].map(map_abbr)\n    df['VisitorTeamAbbr'] = df['VisitorTeamAbbr'].map(map_abbr)\n    \n    df['ToLeft'] = df.PlayDirection == \"left\"\n    # Match the NFLId to that play's rusher's ID\n    df['IsRusher'] = df.NflId == df.NflIdRusher \n    \n    # New feature to show Dir in radians\n    df['Dir_rad'] = np.mod(90 - df.Dir, 360) * math.pi/180.0\n    df['TeamOnOffense'] = \"home\"\n    df.loc[df.PossessionTeam != df.HomeTeamAbbr, 'TeamOnOffense'] = \"away\"\n    \n    # IsOnOffense\n    df['IsOnOffense'] = df.Team == df.TeamOnOffense \n    \n    df['YardLine_std'] = 100 - df.YardLine\n    df.loc[df.FieldPosition.fillna('') == df.PossessionTeam,  \n              'YardLine_std'\n             ] = df.loc[df.FieldPosition.fillna('') == df.PossessionTeam,  \n              'YardLine']\n    df['X_std'] = df.X\n    df.loc[df.ToLeft, 'X_std'] = 120 - df.loc[df.ToLeft, 'X'] \n    df['Y_std'] = df.Y\n    df.loc[df.ToLeft, 'Y_std'] = 160/3 - df.loc[df.ToLeft, 'Y'] \n    df['Orientation_std'] = -90 + df.Orientation\n    df['Dir_std'] = df['Dir_rad']\n    df.loc[df.ToLeft, 'Dir_std'] = np.mod(np.pi + df.loc[df.ToLeft, 'Dir_rad'], 2*np.pi)\n    \n    # Clean Position feature\n    df['Position'] = df['Position'].apply(clean_position)\n    \n    # IsOffenseAtHome\n    df['IsOffenseAtHome'] = True\n    df.loc[df.TeamOnOffense == 'away', 'IsOffenseAtHome'] = False\n    \n    # OffenseTeam & DefenseTeam\n    df['OffenseTeam'] = df['VisitorTeamAbbr']\n    df.loc[df.TeamOnOffense == 'home', 'OffenseTeam'] = df['HomeTeamAbbr']    \n    df['DefenseTeam'] = df['VisitorTeamAbbr']\n    df.loc[df.TeamOnOffense == 'away', 'DefenseTeam'] = df['HomeTeamAbbr']\n    \n    # OffenseScore\n    df['OffenseScore'] = df['HomeScoreBeforePlay']\n    df.loc[df.TeamOnOffense == 'away', 'OffenseScore'] = df['VisitorScoreBeforePlay']\n    \n    # DefenseScore\n    df['DefenseScore'] = df['VisitorScoreBeforePlay']\n    df.loc[df.TeamOnOffense == 'away', 'DefenseScore'] = df['HomeScoreBeforePlay']\n    \n    # IsOffenseWinning\n    df['IsOffenseWinning'] = False\n    df.loc[df.OffenseScore > df.DefenseScore, 'IsOffenseWinning'] = True\n    \n    # OffenseInOwnTerritory\n    df['OffenseInOwnTerritory'] = False\n    df.loc[df.FieldPosition == df.OffenseTeam, 'OffenseInOwnTerritory'] = True\n    \n    # OffenseRushingPosition\n    play_rushers = df.loc[df.NflIdRusher == df.NflId, ['PlayId', 'Position']]\n    play_rushers = play_rushers.rename(columns={'Position': 'OffenseRushingPosition'})\n    df = df.merge(play_rushers, how='outer', left_on='PlayId', right_on='PlayId')\n    \n    # OffenceFormation\n    df['OffenseFormation'] = df['OffenseFormation'].apply(clean_offenceformation)\n    df['OffenseFormation'] = df['OffenseFormation'].fillna(7)  \n    \n    \n    # NumberOfTEsOnPlay, NumberOfWRsOnPlay, NumberOfBacksOnPlay, ....\n    df['GeneralPosition'] = df['Position'].apply(create_generalposition)\n    df['NumberOfBacksOnPlay'] = 0\n    df['NumberOfOLinemenOnPlay'] = 0\n    df['NumberOfWRsOnPlay'] = 0\n    df['NumberOfTEsOnPlay'] = 0\n    df['NumberOfDBsOnPlay'] = 0\n    df['NumberOfDLinemenOnPlay'] = 0 \n    df['NumberOfLBsOnPlay'] = 0\n    # Pivot to find counts of each general position\n    gen_pos_counts = df[['PlayId','GeneralPosition']].pivot_table(index='PlayId', columns='GeneralPosition', \n                                                                  aggfunc=len, fill_value=0)\n    \n    gen_pos_counts = gen_pos_counts.rename(columns = \n                          {'DB':'NumberOfDBsOnPlay', 'DL':'NumberOfDLinemenOnPlay', \n                           'LB':'NumberOfLBsOnPlay', 'OB':'NumberOfBacksOnPlay',\n                           'OL':'NumberOfOLinemenOnPlay', 'TE':'NumberOfTEsOnPlay',\n                           'WR':'NumberOfWRsOnPlay'})\n    gen_pos_counts = gen_pos_counts.reset_index(drop=False)\n    del gen_pos_counts.columns.name\n    gen_pos_counts_cols = gen_pos_counts.columns.values.tolist()\n    gen_pos_counts = gen_pos_counts.loc[gen_pos_counts.index.repeat(22)].reset_index(drop=True)\n    df.update(gen_pos_counts)\n    \n    \n    # DefendersInTheBox\n    df['DefendersInTheBox'] = df['DefendersInTheBox'].fillna(df['DefendersInTheBox'].median())\n    \n    # TimeBetweenSnapHandoff, Month, ...\n    df['TimeBetweenSnapHandoff'] = df['TimeHandoff'].apply(utc2sec) - df['TimeSnap'].apply(utc2sec)\n    df['MatchDay'] = df['TimeSnap'].apply(timesnap2day)\n    df['DayOfYear'] = pd.to_datetime(df['TimeSnap'].apply(timesnap2date)).dt.dayofyear\n    df['DayOfWeek'] = pd.to_datetime(df['TimeSnap'].apply(timesnap2date)).dt.dayofweek\n    df['MonthOfYear'] = df['TimeSnap'].apply(lambda x : int(x[5:7]))\n    df['Morning'] = df['TimeSnap'].apply(lambda x : 1 if (int(x[11:13]) >=0 and int(x[11:13]) <12) else 0)\n    df['Afternoon'] = df['TimeSnap'].apply(lambda x : 1 if (int(x[11:13]) <18 and int(x[11:13]) >=12) else 0)\n    df['Evening'] = df['TimeSnap'].apply(lambda x : 1 if (int(x[11:13]) >= 18 and int(x[11:13]) < 24) else 0)\n    \n    # QuarterGameSecs, TotalGameSecsPlayed, HalfGameSecs\n    df['QuarterGameSecs'] = df['GameClock'].apply(gameclock2secs)\n    df['TotalGameSecsPlayed'] = (900 - df['QuarterGameSecs']) + ((df['Quarter'] - 1) * 900)\n    df['HalfGameSecsLeft'] = df['QuarterGameSecs']\n    df.loc[(df['Quarter'].isin([1,3])), 'HalfGameSecsLeft'] = (900 + df['QuarterGameSecs'])\n    \n    # IsInEngland\n    df['IsInEngland'] = df[\"Location\"].str.lower().map(lambda x: True if \"london\" in x else False)\n    \n    # StadiumType\n    # from https://www.kaggle.com/code1110/optimizing-lightgbm-hyperparameters\n    df['StadiumType'] = df['StadiumType'].apply(group_stadium_types)\n    \n    # Turf\n    # from https://www.kaggle.com/c/nfl-big-data-bowl-2020/discussion/112681#latest-649087\n    df['Turf'] = df['Turf'].map(Turf)\n    df['Turf'] = df['Turf'].map({\"Natural\": 0,\"Artificial\": 1})\n    \n    # GameWeather\n    # https://www.kaggle.com/code1110/optimizing-lightgbm-hyperparameters\n    #df['GameWeather'] = df['GameWeather'].apply(group_game_weather)\n    df['GameWeather'] = df['GameWeather'].str.lower()\n    indoor = \"indoor\"\n    df['GameWeather'] = df['GameWeather'].apply(lambda x: indoor if not pd.isna(x) and indoor in x else x)\n    df['GameWeather'] = df['GameWeather'].apply(lambda x: x.replace('coudy', 'cloudy').replace('clouidy', 'cloudy').replace('party', 'partly') if not pd.isna(x) else x)\n    df['GameWeather'] = df['GameWeather'].apply(lambda x: x.replace('clear and sunny', 'sunny and clear') if not pd.isna(x) else x)\n    df['GameWeather'] = df['GameWeather'].apply(lambda x: x.replace('skies', '').replace(\"mostly\", \"\").strip() if not pd.isna(x) else x)\n    df['GameWeather'] = df['GameWeather'].apply(map_weather)\n    \n        \n    # Temperature \n    df['Temperature'] = df['Temperature'].fillna(df['Temperature'].median())\n    \n    # Humidity\n    df['Humidity'] = df['Humidity'].fillna(df['Humidity'].median())\n    \n    # WindSpeed\n    #df['WindSpeed'] = df['WindSpeed'].apply(clean_wind_speed)\n    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)\n    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))/2 if not pd.isna(x) and '-' in x else x)\n    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)\n    df['WindSpeed'] = df['WindSpeed'].apply(str_to_float)\n    \n    # WindDirection\n    df['WindDirection'] = df['WindDirection'].apply(clean_WindDirection)    \n    df['WindDirection'] = df['WindDirection'].apply(transform_WindDirection)\n    \n    # Team\n    df['Team'] = df['Team'].map({\"home\": 0, \"away\": 1})\n    \n    # Dir\n    df[\"Dir\"] = np.mod(90 - df[\"Dir\"].values, 360)\n    \n    # PlayerBirthDate\n    df['PlayerBirthDate'] = df['PlayerBirthDate'].apply(birthday2day)\n    \n    # PlayerAge\n    df['PlayerAge'] = df['MatchDay'] - df['PlayerBirthDate']\n    \n    # PlayDirection\n    df['PlayDirection'] = df['PlayDirection'].apply(lambda x: x.strip() == 'right')\n    \n    # PlayerWeight\n    df['PlayerHeight'] = df['PlayerHeight'].apply(height2inch)\n    \n    # PlayerBMI\n    df['PlayerBMI'] = df['PlayerWeight'] / df['PlayerHeight']\n    \n    # DefendersInTheBox_vs_Distance\n    # from https://www.kaggle.com/ryches/model-free-benchmark\n    df['DefendersInTheBox_vs_Distance'] = df['DefendersInTheBox'] / df['Distance']\n    \n    # Start\n    # from https://www.kaggle.com/sryo188558/cox-proportional-hazard-model\n    df[\"Start\"] = df[\"YardLine\"]\n    df.loc[(df[\"OffenseInOwnTerritory\"] == 1) & (df[\"PlayDirection\"] == 1), \"Start\"] = df.loc[(df[\"OffenseInOwnTerritory\"] == 1) & (df[\"PlayDirection\"] == 1), \n                                                                                       \"YardLine\"] + 10\n    df.loc[(df[\"OffenseInOwnTerritory\"] == 1) & (df[\"PlayDirection\"] == -1), \"Start\"] = 120 - df.loc[(df[\"OffenseInOwnTerritory\"] == 1) & (df[\"PlayDirection\"] == -1), \n                                                                                       \"YardLine\"] - 10\n    df.loc[(df[\"OffenseInOwnTerritory\"] == 0) & (df[\"PlayDirection\"] == 1), \"Start\"] = 120 - df.loc[(df[\"OffenseInOwnTerritory\"] == 0) & (df[\"PlayDirection\"] == 1), \n                                                                                       \"YardLine\"] - 10\n    df.loc[(df[\"OffenseInOwnTerritory\"] == 0) & (df[\"PlayDirection\"] == -1), \"Start\"] = df.loc[(df[\"OffenseInOwnTerritory\"] == 0) & (df[\"PlayDirection\"] == -1), \n                                                                                       \"YardLine\"] + 10\n    # Orientation \n    df['Orientation'] = 2 * np.pi * (90 - df['Orientation']) / 360\n    \n    # locX\n    #df['locX'] = (df['X'].values - df['Start'].values) * df['PlayDirection'].values\n    #\n    ## locY\n    #df['locY'] = df['Y'].values - 53.3 / 2\n    #\n    ## velX\n    #df['velX'] = df['S'].values * np.cos(df['Orientation'].values) * df['PlayDirection'].values\n    #\n    ## velY\n    #df['velY'] = df['S'].values * np.sin(df['Orientation'].values)\n    #\n    ## accX\n    #df['accX'] = df['A'].values * np.cos(df['Orientation'].values) * df['PlayDirection'].values\n    #\n    ## accY\n    #df['accY'] = df['A'].values * np.sin(df['Orientation'].values)\n    \n    # HomeField\n    df['HomeField'] = df['FieldPosition'] == df['HomeTeamAbbr']\n    \n    # YardsLeft\n    df['YardsLeft'] = df.apply(lambda row: 100-row['YardLine'] if row['HomeField'] else row['YardLine'], axis=1)\n    df['YardsLeft'] = df.apply(lambda row: row['YardsLeft'] if row['PlayDirection'] else 100-row['YardsLeft'], axis=1)    \n    \n\n    df = df.drop(['NflId', 'NflIdRusher','TimeHandoff','TimeSnap','PlayerBirthDate',\n                 'Dir_rad','IsOnOffense','X_std','Y_std','Orientation_std','Dir_std'], axis=1)\n    \n    df = df.select_dtypes(exclude=['object'])\n    \n    #df.drop(df.index[(df['YardsLeft']<df['Yards']) | (df['YardsLeft']-100>df['Yards'])], inplace=True)\n    \n    df = df.sort_values(by=['PlayId', 'Team', 'IsRusher', 'JerseyNumber']).reset_index()\n    df.drop(['GameId', 'PlayId', 'index', 'IsRusher', 'Team'], axis=1, inplace=True)\n    \n    df.fillna(-999, inplace=True)\n\n    return(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = preprocess(df)\ntrain_cols = train.columns\n\nprint(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find cols that are not uniform in each play, append to players_col\nplayers_col = []\nfor col in train.columns:\n    if train[col][:22].std()!=0:\n        players_col.append(col)\n\nX_train = train.drop('Yards', axis=1)\nX_train = np.array(X_train[players_col]).reshape(-1, len(players_col)*22)\n\nplay_col = train.drop(players_col+['Yards'], axis=1).columns\nX_play_col = np.zeros(shape=(X_train.shape[0], len(play_col)))\nfor i, col in enumerate(play_col):\n    X_play_col[:, i] = train[col][::22]\n \nX_train = np.concatenate([X_train, X_play_col], axis=1)\ny_train = np.zeros(shape=(X_train.shape[0], 199))\nfor i,yard in enumerate(train['Yards'][::22]):\n    y_train[i, yard+99:] = np.ones(shape=(1, 100-yard))\n    \nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"players_col length & type: \", len(players_col), type(players_col))\nprint(\"play_col shape & type: \", play_col.shape, type(play_col))\nprint(\"X_play_col shape & type: \", X_play_col.shape, type(X_play_col))\nprint(\"X_train shape & type: \", X_train.shape, type(X_train))\nprint(\"y_train shape & type: \", y_train.shape, type(y_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=64\n\nclass RAdam(keras.optimizers.Optimizer):\n    \"\"\"RAdam optimizer.\n    # Arguments\n        learning_rate: float >= 0. Learning rate.\n        beta_1: float, 0 < beta < 1. Generally close to 1.\n        beta_2: float, 0 < beta < 1. Generally close to 1.\n        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n        decay: float >= 0. Learning rate decay over each update.\n        weight_decay: float >= 0. Weight decay for each param.\n        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n            algorithm from the paper \"On the Convergence of Adam and\n            Beyond\".\n        total_steps: int >= 0. Total number of training steps. Enable warmup by setting a positive value.\n        warmup_proportion: 0 < warmup_proportion < 1. The proportion of increasing steps.\n        min_lr: float >= 0. Minimum learning rate after warmup.\n    # References\n        - [Adam - A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980v8)\n        - [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)\n        - [On The Variance Of The Adaptive Learning Rate And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf)\n    \"\"\"\n\n    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999,\n                 epsilon=None, decay=0., weight_decay=0., amsgrad=False,\n                 total_steps=0, warmup_proportion=0.1, min_lr=0., **kwargs):\n        learning_rate = kwargs.pop('lr', learning_rate)\n        super(RAdam, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.learning_rate = K.variable(learning_rate, name='learning_rate')\n            self.beta_1 = K.variable(beta_1, name='beta_1')\n            self.beta_2 = K.variable(beta_2, name='beta_2')\n            self.decay = K.variable(decay, name='decay')\n            self.weight_decay = K.variable(weight_decay, name='weight_decay')\n            self.total_steps = K.variable(total_steps, name='total_steps')\n            self.warmup_proportion = K.variable(warmup_proportion, name='warmup_proportion')\n            self.min_lr = K.variable(min_lr, name='min_lr')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.initial_decay = decay\n        self.initial_weight_decay = weight_decay\n        self.initial_total_steps = total_steps\n        self.amsgrad = amsgrad\n\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n\n        if self.initial_decay > 0:\n            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n\n        if self.initial_total_steps > 0:\n            warmup_steps = self.total_steps * self.warmup_proportion\n            decay_steps = K.maximum(self.total_steps - warmup_steps, 1)\n            decay_rate = (self.min_lr - lr) / decay_steps\n            lr = K.switch(\n                t <= warmup_steps,\n                lr * (t / warmup_steps),\n                lr + decay_rate * K.minimum(t - warmup_steps, decay_steps),\n            )\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='m_' + str(i)) for (i, p) in enumerate(params)]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='v_' + str(i)) for (i, p) in enumerate(params)]\n\n        if self.amsgrad:\n            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='vhat_' + str(i)) for (i, p) in enumerate(params)]\n        else:\n            vhats = [K.zeros(1, name='vhat_' + str(i)) for i in range(len(params))]\n\n        self.weights = [self.iterations] + ms + vs + vhats\n\n        beta_1_t = K.pow(self.beta_1, t)\n        beta_2_t = K.pow(self.beta_2, t)\n\n        sma_inf = 2.0 / (1.0 - self.beta_2) - 1.0\n        sma_t = sma_inf - 2.0 * t * beta_2_t / (1.0 - beta_2_t)\n\n        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n\n            m_corr_t = m_t / (1.0 - beta_1_t)\n            if self.amsgrad:\n                vhat_t = K.maximum(vhat, v_t)\n                v_corr_t = K.sqrt(vhat_t / (1.0 - beta_2_t))\n                self.updates.append(K.update(vhat, vhat_t))\n            else:\n                v_corr_t = K.sqrt(v_t / (1.0 - beta_2_t))\n\n            r_t = K.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n                         (sma_t - 2.0) / (sma_inf - 2.0) *\n                         sma_inf / sma_t)\n\n            p_t = K.switch(sma_t >= 5, r_t * m_corr_t / (v_corr_t + self.epsilon), m_corr_t)\n\n            if self.initial_weight_decay > 0:\n                p_t += self.weight_decay * p\n\n            p_t = p - lr * p_t\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    @property\n    def lr(self):\n        return self.learning_rate\n\n    @lr.setter\n    def lr(self, learning_rate):\n        self.learning_rate = learning_rate\n\n    def get_config(self):\n        config = {\n            'learning_rate': float(K.get_value(self.learning_rate)),\n            'beta_1': float(K.get_value(self.beta_1)),\n            'beta_2': float(K.get_value(self.beta_2)),\n            'decay': float(K.get_value(self.decay)),\n            'weight_decay': float(K.get_value(self.weight_decay)),\n            'epsilon': self.epsilon,\n            'amsgrad': self.amsgrad,\n            'total_steps': float(K.get_value(self.total_steps)),\n            'warmup_proportion': float(K.get_value(self.warmup_proportion)),\n            'min_lr': float(K.get_value(self.min_lr)),\n        }\n        base_config = super(RAdam, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from https://www.kaggle.com/davidcairuz/nfl-neural-network-w-softmax\ndef crps(y_true, y_pred):\n    return K.mean(K.square(y_true - K.cumsum(y_pred, axis=1)), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n    x = keras.layers.Input(shape=[X_train.shape[1]])\n    fc1 = keras.layers.Dense(units=450, input_shape=[X_train.shape[1]])(x)\n    act1 = keras.layers.PReLU()(fc1)\n    bn1 = keras.layers.BatchNormalization()(act1)\n    dp1 = keras.layers.Dropout(0.55)(bn1)\n    gn1 = keras.layers.GaussianNoise(0.15)(dp1)\n    concat1 = keras.layers.Concatenate()([x, gn1])\n    fc2 = keras.layers.Dense(units=600)(concat1)\n    act2 = keras.layers.PReLU()(fc2)\n    bn2 = keras.layers.BatchNormalization()(act2)\n    dp2 = keras.layers.Dropout(0.55)(bn2)\n    gn2 = keras.layers.GaussianNoise(0.15)(dp2)\n    concat2 = keras.layers.Concatenate()([concat1, gn2])\n    fc3 = keras.layers.Dense(units=400)(concat2)\n    act3 = keras.layers.PReLU()(fc3)\n    bn3 = keras.layers.BatchNormalization()(act3)\n    dp3 = keras.layers.Dropout(0.55)(bn3)\n    gn3 = keras.layers.GaussianNoise(0.15)(dp3)\n    concat3 = keras.layers.Concatenate([concat2, gn3])\n    output = keras.layers.Dense(units=199, activation='softmax')(concat2)\n    model = keras.models.Model(inputs=[x], outputs=[output])\n    return model\n\n\ndef train_model(X_train, y_train, X_val, y_val):\n    model = get_model()\n    model.compile(optimizer=RAdam(warmup_proportion=0.1, min_lr=1e-7), loss=crps)\n    er = EarlyStopping(patience=20, min_delta=1e-4, restore_best_weights=True, monitor='val_loss')\n    model.fit(X_train, y_train, epochs=200, callbacks=[er], validation_data=[X_val, y_val], batch_size=batch_size)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RepeatedKFold\n\nrkf = RepeatedKFold(n_splits=5, n_repeats=5)\n\nmodels = []\n\nfor tr_idx, vl_idx in rkf.split(X_train, y_train):\n    \n    x_tr, y_tr = X_train[tr_idx], y_train[tr_idx]\n    x_vl, y_vl = X_train[vl_idx], y_train[vl_idx]\n    \n    model = train_model(x_tr, y_tr, x_vl, y_vl)\n    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.mean(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_pred(df, sample, env, models):\n    #print('#### BEGINNING #### ')\n    #print('Raw df shape: ', df.shape)\n    #print('Running df preprocessing...')\n    df = preprocess(df)\n    #df_cols = df.columns\n\n    #print('df preprocessing complete')\n    #print('df shape: ', df.shape)\n    #print(' ')\n    \n    #print('Difference between train and df cols')\n    #col_diff = list(set(train_cols) - set(df_cols))\n    #print(col_diff)\n    #print(' ')\n    \n    #print('Reshaping X and removing players_col...')\n    X = np.array(df[players_col]).reshape(-1, len(players_col)*22)\n    #print('Reshape complete')\n    #print('X shape: ', X.shape)\n    #print(' ')\n\n    \n    #print('Defining play_col...')\n    play_col = df.drop(players_col, axis=1).columns\n    #print('play_col shape: ', play_col.shape)\n    #print(' ')\n    \n    \n    #print('Defining X_play_col...')\n    X_play_col = np.zeros(shape=(X.shape[0], len(play_col)))\n    #print('X_play_col shape(1): ', X_play_col.shape)\n    #print(' ')\n\n    \n    #print('Selecting X_play_col data...')\n    for i, col in enumerate(play_col):\n        X_play_col[:, i] = df[col][::22]\n    #print('X_play_col shape(2): ', X_play_col.shape)\n    #print(' ')\n    \n    \n    #print('Concatenating X with X_play_col...')\n    X = np.concatenate([X, X_play_col], axis=1)\n    #print('X concatenation complete')\n    #print('X shape: ', X.shape)\n    #print(' ')\n\n    \n    #print('Scaling X...')\n    X = scaler.transform(X)\n    #print('Scaling X complete')\n    #print('X shape: ', X.shape)\n    #print(' ')\n\n    \n    #print('Predicting y_pred...')    \n    y_pred = np.mean([np.cumsum(model.predict(X), axis=1) for model in models], axis=0)\n    #print('Prediction complete')\n    #print('y_pred shape: ', y_pred.shape)\n    #print(' ')\n\n    \n    #print('Defining YardsLeft...')\n    yardsleft = np.array(df['YardsLeft'][::22])\n    #print('YardsLeft shape: ', yardsleft.shape)\n    #print(' ')\n\n    \n    #print('Looping y_pred for final sub...')\n    for i in range(len(yardsleft)):\n        y_pred[i, :yardsleft[i]-1] = 0\n        y_pred[i, yardsleft[i]+100:] = 1\n    #print('y_pred loop complete')\n    #print('y_pred shape: ', y_pred.shape)\n    #print(' ')\n\n    \n    #print('Finalising submission')\n    env.predict(pd.DataFrame(data=y_pred.clip(0,1),columns=sample.columns))\n    #print('Submission complete. Moving to next iteration.')\n    #print('#### END ####')\n    #print(' ')\n    #print(' ')\n    #print(y_pred)\n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle.competitions import nflrush\nenv = nflrush.make_env()\n\nfor test, sample in tqdm.tqdm(env.iter_test()):\n    make_pred(test, sample, env, models)\nenv.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## from https://www.kaggle.com/newbielch/lgbm-regression-view\n#def get_cdf_df(yards_array):\n#    pdf, edges = np.histogram(yards_array, bins=199,\n#                 range=(-99,100), density=True)\n#    cdf = pdf.cumsum().clip(0, 1)\n#    cdf_df = pd.DataFrame(data=cdf.reshape(-1, 1).T, \n#                            columns=['Yards'+str(i) for i in range(-99,100)])\n#    return cdf_df\n#cdf = get_cdf_df(y_train).values.reshape(-1,)\n##dist_to_end_train = X_train.apply(lambda x:(100 - x.loc['YardLine']) if x.loc[\"OffenseInOwnTerritory\"]==1 else x.loc['YardLine'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#def get_score(y_pred,cdf,w,dist_to_end):\n#    y_pred = int(y_pred)\n##     y_pred = y_pred.astype(int)\n#    if y_pred ==w:\n#        y_pred_array = cdf.copy()\n#    elif y_pred - w >0:\n#        y_pred_array = np.zeros(199)\n#        y_pred_array[(y_pred-w):] = cdf[:(-(y_pred-w))].copy()\n#    elif w - y_pred >0:\n#        y_pred_array = np.ones(199)\n#        y_pred_array[:(y_pred-w)] = cdf[(w-y_pred):].copy()\n#    y_pred_array[-1]=1\n#    y_pred_array[(dist_to_end+99):]=1\n#    return y_pred_array    \n#\n#def get_score_pingyi1(y_pred,y_true,cdf,w,dist_to_end):\n#    y_pred = int(y_pred)\n#    if y_pred ==w:\n#        y_pred_array = cdf.copy()\n#    elif y_pred - w >0:\n#        y_pred_array = np.zeros(199)\n#        y_pred_array[(y_pred-w):] = cdf[:(-(y_pred-w))].copy()\n#    elif w - y_pred >0:\n#        y_pred_array = np.ones(199)\n#        y_pred_array[:(y_pred-w)] = cdf[(w-y_pred):].copy()\n#    y_pred_array[-1]=1\n#    y_pred_array[(dist_to_end+99):]=1\n#    y_true_array = np.zeros(199)\n#    y_true_array[(y_true+99):]=1\n#    return np.mean((y_pred_array - y_true_array)**2)\n#\n#\n#def CRPS_pingyi1(y_preds,y_trues,w,cdf,dist_to_ends):\n#    if len(y_preds) != len(y_trues):\n#        print('length does not match')\n#        return None\n#    n = len(y_preds)\n#    tmp = []\n#    for a,b,c in zip(y_preds, y_trues, dist_to_ends):\n#        tmp.append(get_score_pingyi1(a,b,cdf,w,c))\n#    return np.mean(tmp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Initial LGB parameters are ...\n#lgbParams = {\n#    'objective': 'regression',\n#    'metric': 'mae',\n#    'verbosity': -1,\n#    'boosting_type': 'gbdt',\n#    \"num_iterations\": 1000, \n#    \"learning_rate\": 0.05,\n#    \"lambda_l1\": 9,\n#    \"lambda_l2\": 0.9,\n#    \"num_leaves\": 42,\n#    \"feature_fraction\": 0.4,\n#    \"bagging_fraction\": 0.45,\n#    \"bagging_freq\": 7,\n#    \"min_child_samples\": 74,\n#    \"random_state\": 14\n#}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Visualize feature importance\n#\n## make a LightGBM dataset\n#trainX, testX, trainY, testY = train_test_split(X_train, y_train, test_size=0.33, random_state=14)\n#d_train = lgb.Dataset(trainX, trainY)\n#d_eval = lgb.Dataset(testX, testY, reference=d_train)\n#\n## model training\n#LGBmodel = lgb.train(lgbParams, d_train, valid_sets=d_eval, verbose_eval=1000)\n## LGBmodel = lgb.train(lgbParams, d_train, valid_sets=d_eval, early_stopping_rounds=500, verbose_eval=1000)\n#\n## feature importance\n#importance = LGBmodel.feature_importance(importance_type=\"gain\")\n#ranking = np.argsort(-importance)\n#fig, ax = plt.subplots(figsize=(20, 20))\n#sns.barplot(x=importance[ranking], y=X_train.columns.values[ranking], orient='h')\n#ax.set_xlabel(\"feature importance\")\n#plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#features = X_train.columns.values[ranking][:30]\n#print(features)\n#X_train = X_train[features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## FYI: Objective functions can take additional arguments\n## (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).\n#def objective(trial):\n#  \n#    # make a LightGBM dataset\n#    trainX, testX, trainY, testY = train_test_split(X_train, y_train, test_size=0.33, random_state=14)\n#    d_train = lgb.Dataset(trainX, trainY)\n#\n#    param = {\n#        'objective': 'regression',\n#        'metric': 'mae',\n#        'verbosity': -1,\n#        'boosting_type': 'gbdt',\n#        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n#        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n#        'num_leaves': trial.suggest_int('num_leaves', 40, 256),\n#        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n#        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n#        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n#        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n#    }\n#\n#    gbm = lgb.train(param, d_train)\n#    preds = gbm.predict(testX)\n#    mae = mean_absolute_error(testY, preds)\n#    return mae","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#study = optuna.create_study(direction='minimize')\n#study.optimize(objective, n_trials=100)\n#\n#print('Number of finished trials: {}'.format(len(study.trials)))\n#\n#print('Best trial:')\n#trial = study.best_trial\n#\n#print('  Value: {}'.format(trial.value))\n#\n#print('  Params: ')\n#for key, value in trial.params.items():\n#    print('    {}: {}'.format(key, value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lgbParams = trial.params\n#lgbParams['objective'] = 'regression'\n#lgbParams['metric'] = 'mae'\n#lgbParams['verbosity'] = -1\n#lgbParams['boosting_type'] = 'gbdt'\n#lgbParams[\"learning_rate\"] = 0.01\n#lgbParams[\"num_iterations\"] = 5000\n#lgbParams[\"random_state\"] = 14\n#print(lgbParams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lgbParams = {'lambda_l1': 9.830445878717612, \n#             'lambda_l2': 9.889595129567367, \n#             'num_leaves': 41, \n#             'feature_fraction': 0.4152661948258711,\n#             'bagging_fraction': 0.9170414824158851, \n#             'bagging_freq': 7, \n#             'min_child_samples': 44, \n#             'objective': 'regression', \n#             'metric': 'mae', \n#             'verbosity': -1, \n#             'boosting_type': 'gbdt', \n#             'learning_rate': 0.01, \n#             'num_iterations': 5000, \n#             'random_state': 14}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#n_splits = 5\n#seed = 14\n#kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n#y_valid = np.zeros(X_train.shape[0])\n#models = []\n#\n#for train_idx, valid_idx in kf.split(X_train, y_train):\n#    trainX, trainY = X_train.iloc[train_idx, :], y_train[train_idx]\n#    validX, validY = X_train.iloc[valid_idx, :], y_train[valid_idx]\n#    \n#    d_train = lgb.Dataset(trainX, trainY)\n#    d_eval = lgb.Dataset(validX, validY, reference=d_train)\n#    \n#    LGBmodel = lgb.train(lgbParams, d_train, valid_sets=d_eval, \n#                         early_stopping_rounds=500, \n#                         learning_rates = lambda iter: 0.01 * (0.99 ** iter),\n#                         verbose_eval = 1000)\n#    y_valid[valid_idx] += LGBmodel.predict(validX, num_iteration=LGBmodel.best_iteration)\n#    models.append(LGBmodel)\n#gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cprs = CRPS_pingyi1(y_valid, y_train.astype(int), 4, cdf, dist_to_end_train.astype(int))\n#print(\"cprs = {}\".format(cprs))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from kaggle.competitions import nflrush\n#env = nflrush.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#index = 0\n#for (test_df, sample_prediction_df) in tqdm.tqdm(env.iter_test()):\n#    test = preprocess(test_df)\n#    \n#    count=0\n#    test_data = np.zeros((1,len(features)))\n#    for c in features:\n#        try:\n#            test_data[0][count] = test[c][index]\n#        except:\n#            test_data[0][count] = np.nan\n#        count+=1\n#    \n#    y_pred = np.zeros(199)    \n#    y_pred_p = np.sum(np.round(scaler.inverse_transform(\n#        [model.predict(test_data) for model in models]))) / n_splits\n#    \n#    y_pred_p += 99\n#    for j in range(199):\n#        if j>=y_pred_p+10:\n#            y_pred[j]=1.0\n#        elif j>=y_pred_p-10:\n#            y_pred[j]=(j+10-y_pred_p)*0.05\n#\n#    env.predict(pd.DataFrame(data=[y_pred],columns=sample_prediction_df.columns))\n#    index += 22\n#env.write_submission_file()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}